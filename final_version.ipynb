{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ahead-district",
   "metadata": {},
   "source": [
    "# Интеграция планирования и обучения с подкреплением\n",
    "## OpenTalks.AI 2021\n",
    "### Кирилл Аксёнов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-exhibit",
   "metadata": {},
   "source": [
    "## Часть 1: Отличия планирования и RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-liability",
   "metadata": {},
   "source": [
    "### RL:\n",
    "В RL зачастую цнль состоит в том, чтобы максимизировать награду (дисконтированное будущее накопленное вознаграждение, или, проще говоря, вознаграждение в долгосрочной перспективе).\n",
    "\n",
    "Другой ключевой особенностью обучения с подкреплением является то, что оно явно рассматривает всю проблему целенаправленного взаимодействия агента с неопределенной средой.\n",
    "\n",
    "В RL есть явное понятие агента, среды и цели, и награда - единственный сигнал, который сообщает агенту, насколько хорошо он работает, но награда не сообщает агенту, какие действия он должен предпринять в каждом конкретном случае.\n",
    "\n",
    "<img src=\"https://github.com/hse-ds/iad-applied-ds/raw/master/2020/seminars/seminar15/rlIntro.png\" caption=\"Взаимодействия агента со средой\" style=\"width: 500px;\" />\n",
    "\n",
    "\n",
    "\n",
    "### Планирование\n",
    "Планирование же часто выполняется «офлайн», то есть перед выполнением. Пока выполняется план, он зачастую остается неизменным. Однако, во многих случаях это нежелательно, поскольку может потребоваться реакция на изменение среды. Ввиду планирования поведения до конца эпизода, агенту гораздо проще достигать долгосрочных наград, особенно когда они разреженны.\n",
    "\n",
    "### RL + Планирование:\n",
    "RL и планирование довольно взаимосвязаны ввиду аналогичности проблем. Однако, в RL модель перехода и функция вознаграждения MDP  обычно неизвестны. Следовательно, единственный способ найти или оценить оптимальную стратегию, которая позволит агенту (почти оптимально) вести себя в этой среде, - это взаимодействовать со средой и собирать некоторую информацию о ее «динамике», после того как появиться модель среды возможно планирование. При таком подходе агент изучает среду в которой находится, для того чтобы в дальнейшем использовать полученные знания для планирования своего поведения с целью достижения максимальной награды."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-chinese",
   "metadata": {},
   "source": [
    "## Часть 2: Среды и оболочки над ними"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-synthesis",
   "metadata": {},
   "source": [
    "Важная часть в обучении с подкреплением это среда!\n",
    "\n",
    "[Gym](https://gym.openai.com) $-$ это набор инструментов для разработки и сравнения алгоритмов обучения с подкреплением, который также включает в себя большой [набор окружений](https://gym.openai.com/envs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-allergy",
   "metadata": {},
   "source": [
    "### Создание окружения\n",
    "\n",
    "Для создания окружения используется функция ```gym.make(<имя окружения>)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Создаем окружение\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Инициализируем окружение\n",
    "state = env.reset()\n",
    "print(f\"state: {state}\")\n",
    "\n",
    "# Выполняем действие в среде \n",
    "next_state, r, done, info = env.step(0)\n",
    "print(f\"next_state: {next_state} , r: {r}, done: {done}, info: {info}\")\n",
    "\n",
    "# Закрываем окружение\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-musical",
   "metadata": {},
   "source": [
    "### Основные методы окружения:\n",
    "\n",
    "* ``reset()`` $-$ инициализация окружения, возвращает первое наблюдение (состояние).  \n",
    "* ``step(a)`` $-$ выполнить в среде действие $\\mathbf{a}$ и получить кортеж: $\\mathbf{\\langle s_{t+1}, r_t, done, info \\rangle}$, где $\\mathbf{s_{t+1}}$ - следующее состояние, $\\mathbf{r_t}$ - вознаграждение, $\\mathbf{done}$ - флаг заверешния, $\\mathbf{info}$ - дополнительная информация\n",
    "\n",
    "### Дополнительные методы:\n",
    "* ``render()`` $-$ визуализация текущего состояния среды (удобно, если мы запускаем локально, в колабе ничего не увидим)\n",
    "\n",
    "* ``close()`` $-$ закрывает окружение "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-behalf",
   "metadata": {},
   "source": [
    "### Свойства среды:\n",
    "* ``env.observation_space`` $-$ информация о пространстве состояний\n",
    "* ``env.action_space`` $-$ информация о пространстве действий\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"env.observation_space: {env.observation_space}\")\n",
    "print(f\"env.action_space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-circumstances",
   "metadata": {},
   "source": [
    "### Среда ``Taxi-v3``\n",
    "\n",
    "Информацию о любой среде можно найти в [исходниках](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) или на [сайте](https://gym.openai.com/envs/MountainCar-v0/). О ``Taxi-v3`` мы можем узнать следующее: \n",
    "\n",
    "\n",
    "#### Задание:\n",
    "Есть 4 места (помечены разными буквами), задача состоит в том, чтобы забрать пассажира в одном месте и высадить его в другом. +20 баллов дается за успешный перевозку пассажира, но за каждый шаг теряется 1 балл. Также существует штраф в размере 10 баллов за неправильную посадку и высадку.\n",
    "\n",
    "#### Пространство состояний  Discrete(500):\n",
    "\n",
    "Имеется 500 дискретных состояний, поскольку имеется 25 мест, где может оказаться такси, 5 возможных местоположений пассажира (включая случай, когда пассажир находится в такси) и 4 места назначения.\n",
    "\n",
    "\n",
    "#### Пространство действий Discrete(6):\n",
    "\n",
    "Num | Action|\n",
    "----|-------------|\n",
    "0   | move north         |\n",
    "1   | move east          |\n",
    "2   | move west          |\n",
    "3   | push left          |\n",
    "4   | pickup passenger   |\n",
    "5   | drop off passenger |\n",
    "\n",
    "* Вознаграждения: \n",
    "     По умолчанию вознаграждение за шаг равно -1,\n",
    "     кроме доставки пассажира(+20),\n",
    "     или незаконное выполнение действий \"посадка\" и \"высадка\"(-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-switzerland",
   "metadata": {},
   "source": [
    "### Пример со случайной стратегией:\n",
    "\n",
    "Для выбора действия используется ``env.action_space.sample()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# создаем окружение\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "# добавляем wrapper (обертку), чтобы задать ограничение на число шагов в среде\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=250)\n",
    "\n",
    "# проводим инициализацию и запоминаем начальное состояние\n",
    "s = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # выполняем действие, получаем s, r, done, info\n",
    "    s, r, done, _ = env.step(env.action_space.sample())\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    \n",
    "env.close()\n",
    "\n",
    "# Сначала закрываем окружение, чтобы видео записалось полностью\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-rating",
   "metadata": {},
   "source": [
    "### Оболочки\n",
    "#### Reward Wrapper\n",
    "Мы знаем, что окружения создаются с помощью команды ``gym.make(<имя среды>)``, но что если мы хотим немного изменить окружение или добавить какой-то дополнительный функционал? Для этого существуют обертки (wrappers). Рассмотрим среду ``Taxi-v3``, предположим, что мы хотим поменять вознаграждения на следующие: 1 за решение задачи, -1 за неправильную посадку/высадку пассажира и 0 во всех остальных случаях. Для этого мы можем воспользоваться ``RewardWrapper``-ом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRewardWrapper(gym.RewardWrapper):\n",
    "\n",
    "    def reward(self, reward):\n",
    "        if reward == -1:\n",
    "            return 0\n",
    "        elif reward == 20:\n",
    "            return 1\n",
    "        elif reward == -10:\n",
    "            return -1\n",
    "        else: \n",
    "            raise KeyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyRewardWrapper(gym.make(\"Taxi-v3\"))\n",
    "observation = env.reset()\n",
    "\n",
    "rewards = set()\n",
    "\n",
    "while True:\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    rewards.add(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# выведем все вознаграждения, которые получал агент\n",
    "print(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-heather",
   "metadata": {},
   "source": [
    "#### Time Limit Wrapper\n",
    "\n",
    "В зависимости от рандома, мы могли получить разные результаты, но обычно это ``{0, -1}``. Откуда такой результат? Ведь среда заканчивается только когда задание выполнено. Все дело во встроенной обертке ограничивающей максимальное количество шагов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "print(type(env))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-nicaragua",
   "metadata": {},
   "source": [
    "Можно воспользоваться окружением без этой обертки, вызвав ``.env``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env = MyRewardWrapper(env)\n",
    "\n",
    "observation = env.reset()\n",
    "rewards = set()\n",
    "\n",
    "while True:\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    rewards.add(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-copying",
   "metadata": {},
   "source": [
    "А если у нас есть окружение без этого враппера по умолчанию, то можно добавить его вот так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env = MyRewardWrapper(env)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=1)\n",
    "\n",
    "observation = env.reset()\n",
    "rewards = set()\n",
    "\n",
    "while True:\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    rewards.add(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-valuable",
   "metadata": {},
   "source": [
    "Чтобы различать два случая завершения среды, в ``info`` передается дополнительный ключ ``TimeLimit.truncated: True``, если среду завершил ``TimeLimit`` враппер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=1)\n",
    "\n",
    "env.reset()\n",
    "_, _, _, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-privilege",
   "metadata": {},
   "source": [
    "### Action Wrapper\n",
    "Представим, что наш водитель находится не в лучшем своем состоянии и независимо от выбора агента, в 50% случаев совершает случайные действий. Сделать среду стохастическои и добиться такого эффекта мы можем, используя ``ActionWrapper``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TaxiRandomActionWrapper(gym.ActionWrapper):\n",
    "\n",
    "    def __init__(self, env, probability=0.5):\n",
    "        super().__init__(env) \n",
    "        self.probability = probability\n",
    "        \n",
    "\n",
    "    def action(self, action):\n",
    "        if np.random.random() < self.probability:\n",
    "            return env.action_space.sample()\n",
    "        else: \n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-sodium",
   "metadata": {},
   "source": [
    "Чтобы проверить, что обертка работает будем выполнять единственное действие:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env = MyRewardWrapper(env)\n",
    "env = TaxiRandomActionWrapper(env)\n",
    "\n",
    "observation = env.reset()\n",
    "rewards = set()\n",
    "\n",
    "while True:\n",
    "    action = 0\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.add(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-cleaning",
   "metadata": {},
   "source": [
    "### Wrapper\n",
    "\n",
    "Класс ``gym.Wrapper`` является базовым для всех оберток. Подкласс может переопределить    многие методы для изменения поведения исходной среды, не изменяя при этом ее исходный код. Например, мы можем изменить метод step и добавить, какую-то дополнительную информацию в ``info``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWrapper(gym.Wrapper):\n",
    "    \n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        info['Wrapped'] = True\n",
    "        return observation, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env = MyWrapper(env)\n",
    "\n",
    "env.reset()\n",
    "_, _, _, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-privilege",
   "metadata": {},
   "source": [
    "## Часть 3: Dyna-Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-indication",
   "metadata": {},
   "source": [
    "Давайте реализуем простейший алгоритм планирования, а именно табличный алгоритм Dyna-Q.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/10/dyna.png\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(rewards_batch, log, reward_range=None):\n",
    "    \"\"\"\n",
    "    Удобная функция, которая отображает прогресс обучения.\n",
    "    \"\"\"\n",
    "\n",
    "    if reward_range is None:\n",
    "        reward_range = [-990, +10]\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    log.append([mean_reward])\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()\n",
    "\n",
    "# гиперпараметры алгоритма\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "episodes_number = 1001\n",
    "on_model_updates = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, env_):\n",
    "        self.mask_state = np.zeros([env_.observation_space.n], dtype=\"int32\")\n",
    "        self.mask_state_action = np.zeros([env_.observation_space.n, env_.action_space.n], dtype=\"int32\")\n",
    "        self.r = np.zeros([env_.observation_space.n, env_.action_space.n])\n",
    "        self.next_s = np.zeros([env_.observation_space.n, env_.action_space.n], dtype=\"int32\")\n",
    "\n",
    "    def add(self, s: int, a: int, r: float, next_s: int):\n",
    "        self.mask_state[s] = 1\n",
    "        self.mask_state_action[s][a] = 1\n",
    "        self.r[s][a] = r \n",
    "        self.next_s[s][a] = next_s\n",
    "\n",
    "    def sample(self) -> [int, int, float, int]:\n",
    "        \"\"\"\n",
    "        returns s, a, r, next_s\n",
    "        \"\"\"\n",
    "        s = np.random.choice(np.where(self.mask_state == 1)[0])\n",
    "        a = np.random.choice(np.where(self.mask_state_action[s] == 1)[0])\n",
    "        return s, a, self.r[s][a], self.next_s[s][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(env):\n",
    "    q_table_ = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    return q_table_\n",
    "\n",
    "# определяем память, в которой будет храниться Q(s,a)\n",
    "q_table = initialize_q_table(env)\n",
    "log = []\n",
    "rewards_batch = []\n",
    "\n",
    "# инициализируем модель\n",
    "m = Model(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q(alpha_, reward_, gamma_, next_max_, old_value_):\n",
    "    return (1 - alpha_) * old_value_ + alpha_ * (reward_ + gamma_ * next_max_)\n",
    "\n",
    "\n",
    "for i in range(1, episodes_number):\n",
    "    state = env.reset()\n",
    "\n",
    "    episode, reward, episode_reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # выбираем действие, используя eps-greedy исследование среды\n",
    "        # с вероятностью epsilon выбираем случайное действие, иначе\n",
    "        # выполняем действие жадно, согласно текущей Q-таблице\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # исследуем среду\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # используем Q-функцию\n",
    "\n",
    "        # выполняем действие в среде\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        m.add(state, action, reward, next_state)\n",
    "        # получаем old_value (Q(s,a)) и next_max (max(Q(s', a')))\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        q_table[state, action] = compute_q(alpha_=alpha, reward_=reward, gamma_=gamma, next_max_=next_max,\n",
    "                                           old_value_=old_value)\n",
    "        state = next_state\n",
    "\n",
    "        for _ in range(on_model_updates):\n",
    "            m_s, m_a, m_r, m_next_s = m.sample()\n",
    "            old_value = q_table[m_s, m_a]\n",
    "            next_max = np.max(q_table[m_next_s])\n",
    "            q_table[m_s, m_a] = compute_q(alpha_=alpha, reward_=m_r, gamma_=gamma, next_max_=next_max,\n",
    "                                          old_value_=old_value)\n",
    "\n",
    "        episode += 1\n",
    "        episode_reward += reward\n",
    "    rewards_batch.append(episode_reward)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        show_progress(rewards_batch, log)\n",
    "        rewards_batch = []\n",
    "        print(f\"Episode: {i}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-tuner",
   "metadata": {},
   "source": [
    "## Часть 4: Monte-Carlo Tree Search(MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-racing",
   "metadata": {},
   "source": [
    "### Среда\n",
    "#### О игре четыре в ряд\n",
    "* Четыре в ряд - это пошаговая игра, в которой два игрока поочередно бросают цветные диски в вертикальную сетку. Цель каждого игрока - сформировать **последовательность из четырех дисков** в ряд первым. Это **игра с полной информацией**, то есть каждый игрок хорошо осведомлен обо всех событиях, которые произошли ранее.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Connect_Four.gif\">\n",
    "\n",
    "\n",
    "* Четыре в ряд также можно рассматривать как **игру с нулевой суммой**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-philosophy",
   "metadata": {},
   "source": [
    "### Среда Kaggle для загрузки ConnectX\n",
    "\n",
    "* Kaggle предоставляет удивительно простую в использовании OpenAI-подобную среду для игры в четыре в ряд.\n",
    "\n",
    "* Для окончательного тестирования агента также возможно использование предопределенных **random** и **negamax** агентов.\n",
    "\n",
    "https://www.kaggle.com/ajeffries/connectx-getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'kaggle-environments>=0.1.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-mayor",
   "metadata": {},
   "source": [
    "### Загрузка среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "env.reset()\n",
    "\n",
    "env.run([\"random\", \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-serum",
   "metadata": {},
   "source": [
    "### Различные подходы к решению проблемы\n",
    "Для решения этой проблемы может быть много разных алгоритмов, например\n",
    "* Minimax (negamax)\n",
    "* Minimax с альфа-бета отсечением\n",
    "* Q Learning\n",
    "* PPO\n",
    "* Monte-Carlo Tree Search\n",
    "* etc\n",
    "\n",
    "Рассмотрим алгоритм поиска по дереву Монте-Карло. Основным узким местом для игр с большим пространством действий (7 в случае connect4) является то, что он требует обширного поиска с учетом различных перестановок и комбинаций игрового поля. MCTS пытается эффективно решить эту проблему, как объясняется ниже, за счет сокращения пространства поиска и в то же время сохранения эффективности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-quantum",
   "metadata": {},
   "source": [
    "### Поиск по дереву Монте-Карло: теория\n",
    "**Идея**: поиск по дереву методом Монте-Карло строит дерево поиска с n узлами, каждый из которых содержит информацию о количестве побед и посещений. Первоначально дерево начинается с одного корневого узла и выполняет итерации до тех пор, пока не закончилось отведенной под поиск время.\n",
    "\n",
    "MCTS состоит из четырех основных этапов\n",
    "<img src = \"https://miro.medium.com/max/6000/1*i4qhdo44WCF1YL25gneWmw.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "* **Выбор** - на этом этапе агент запускается с корневого узла, выбирает узел, используя стратегию по дереву (классический вариант подразумевает эпсилон-жадную), применяет выбранные действия и продолжает работу, пока не будет достигнуто конечное состояние. В нашем случае будем использовать эпсилон жадную стратегию.\n",
    "* **Расширение** - когда найден узел, который до этого не посещался, дерево расширяется, чтобы включить неисследованный узел.\n",
    "* **Симуляция** - после добавления узла алгоритм доигрывает партию(игру) до конца, при этом используется быстрая стратегия (зачастую случайная).\n",
    "* **Обратное распространение** - когда агент достигает конечного состояния игры, обновляются все пройденные узлы, а именно статистика посещений и побед для каждого узла.\n",
    "\n",
    "Вышеупомянутые шаги повторяются для некоторых итераций.\n",
    "\n",
    "* Наконец, в качестве следующего действия выбирается дочерний элемент корневого узла с наибольшим числом посещений, поскольку чем больше число посещений, тем выше значение ucb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-encounter",
   "metadata": {},
   "source": [
    "### Работа MCTS\n",
    "* Каждая итерация начинается с корня.\n",
    "* Следуя стратегии по дереву, достигаем листового узла.\n",
    "* Добавляем узел \"N\".\n",
    "* Доигрываем партию используя случайную стратегию.\n",
    "* Обновляем статистику в каждом посещенном узле."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-limitation",
   "metadata": {},
   "source": [
    "### MCTS Agent to play connectX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectx_agent(obs, config):\n",
    "    \n",
    "    def put_new_piece(grid, col, mark, config):\n",
    "        next_state = grid.copy()\n",
    "        for row in range(config.rows-1, -1, -1):\n",
    "            if not next_state[row][col]:\n",
    "                break\n",
    "        next_state[row][col] = mark\n",
    "        return next_state\n",
    "\n",
    "    def check_result(grid, piece, config):\n",
    "\n",
    "        def look_for_window(window):\n",
    "            return (window.count(piece) == 4 and window.count(0) == config.inarow - 4)\n",
    "\n",
    "        def calculate_windows():\n",
    "            is_success = False\n",
    "            sequences = ['horizontal', 'vertical', 'p_diagonal', 'n_diagonal']\n",
    "\n",
    "            for sequence_type in sequences:\n",
    "\n",
    "                if sequence_type == 'horizontal':\n",
    "                    for row in range(config.rows):\n",
    "                        for col in range(config.columns-(config.inarow-1)):\n",
    "                            window = list(grid[row, col:col+config.inarow])\n",
    "                            if look_for_window(window):\n",
    "                                return True\n",
    "\n",
    "                elif sequence_type == 'vertical':\n",
    "                    for row in range(config.rows-(config.inarow-1)):\n",
    "                        for col in range(config.columns):\n",
    "                            window = list(grid[row:row+config.inarow, col])\n",
    "                            if look_for_window(window):\n",
    "                                return True\n",
    "\n",
    "                elif sequence_type == 'p_diagonal':\n",
    "                    for row in range(config.rows-(config.inarow-1)):\n",
    "                        for col in range(config.columns-(config.inarow-1)):\n",
    "                            window = list(\n",
    "                                grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "                            if look_for_window(window):\n",
    "                                return True\n",
    "\n",
    "                elif sequence_type == 'n_diagonal':\n",
    "                    for row in range(config.inarow-1, config.rows):\n",
    "                        for col in range(config.columns-(config.inarow-1)):\n",
    "                            window = list(\n",
    "                                grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "                            if look_for_window(window):\n",
    "                                return True\n",
    "\n",
    "            return is_success\n",
    "\n",
    "        return calculate_windows()\n",
    "\n",
    "    class MCTS():\n",
    "\n",
    "        def __init__(self, obs, config):\n",
    "\n",
    "            self.state = np.asarray(obs.board).reshape(\n",
    "                config.rows, config.columns)\n",
    "            self.config = config\n",
    "            self.player = obs.mark\n",
    "            self.final_action = None\n",
    "            self.time_limit = 4\n",
    "            self.root_node = (0,)\n",
    "            self.tunable_constant = 1.0\n",
    "            self.value_type = 'epsilon-greedy'\n",
    "            self.epsilon = 0.1\n",
    "\n",
    "            self.tree = {self.root_node: {'state': self.state, 'player': self.player,\n",
    "                                          'child': [], 'parent': None, 'total_node_visits': 0,\n",
    "                                          'total_node_wins': 0}}\n",
    "            self.total_parent_node_visits = 0\n",
    "            \n",
    "        def get_value(self, node_no):\n",
    "            if not self.total_parent_node_visits:\n",
    "                return math.inf\n",
    "            else:\n",
    "                if self.value_type == 'greedy' or self.value_type == 'epsilon-greedy':\n",
    "                    value_estimate = self.tree[node_no]['total_node_wins'] / \\\n",
    "                        (self.tree[node_no]['total_node_visits'] + 1)\n",
    "                    return value_estimate\n",
    "#                 elif self.value_type == 'ucb':\n",
    "\n",
    "\n",
    "        def selection(self):\n",
    "            '''\n",
    "            Aim - To select the leaf node with the maximum value\n",
    "            '''\n",
    "            is_terminal_state = False\n",
    "            leaf_node_id = (0,)\n",
    "            while not is_terminal_state:\n",
    "                node_id = leaf_node_id\n",
    "                number_of_child = len(self.tree[node_id]['child'])\n",
    "                if not number_of_child:\n",
    "                    leaf_node_id = node_id\n",
    "                    is_terminal_state = True\n",
    "                else:\n",
    "                    max_value_score = -math.inf\n",
    "                    best_action = leaf_node_id\n",
    "                    for i in range(number_of_child):\n",
    "                        action = self.tree[node_id]['child'][i]\n",
    "                        child_id = leaf_node_id + (action,)\n",
    "                        current_value = self.get_value(child_id)\n",
    "                        if current_value > max_value_score:\n",
    "                            max_value_score = current_value\n",
    "                            best_action = action\n",
    "                    if self.value_type == 'epsilon-greedy' and np.random.sample() < self.epsilon:\n",
    "                        current_state = self.tree[leaf_node_id]['state']\n",
    "                        current_board = np.asarray(current_state).reshape(config.rows*config.columns)\n",
    "                        self.actions_available = [c for c in range(self.config.columns) if not current_board[c]]\n",
    "                        rand_act = np.random.choice(self.actions_available)\n",
    "#                         print(rand_act)\n",
    "                        leaf_node_id = leaf_node_id + (rand_act,)\n",
    "                    else:\n",
    "                        leaf_node_id = leaf_node_id + (best_action,)\n",
    "            return leaf_node_id\n",
    "\n",
    "        def expansion(self, leaf_node_id):\n",
    "            '''\n",
    "            Aim - Add new nodes to the current leaf node by taking a random action\n",
    "                  and then take a random or follow any policy to take opponent's action.\n",
    "            '''\n",
    "            current_state = self.tree[leaf_node_id]['state']\n",
    "            player_mark = self.tree[leaf_node_id]['player']\n",
    "            current_board = np.asarray(current_state).reshape(\n",
    "                config.rows*config.columns)\n",
    "            self.actions_available = [c for c in range(\n",
    "                self.config.columns) if not current_board[c]]\n",
    "            done = check_result(current_state, player_mark, self.config)\n",
    "            child_node_id = leaf_node_id\n",
    "            is_availaible = False\n",
    "\n",
    "            if len(self.actions_available) and not done:\n",
    "                childs = []\n",
    "                for action in self.actions_available:\n",
    "                    child_id = leaf_node_id + (action,)\n",
    "                    childs.append(action)\n",
    "                    new_board = put_new_piece(\n",
    "                        current_state, action, player_mark, self.config)\n",
    "                    self.tree[child_id] = {'state': new_board, 'player': player_mark,\n",
    "                                           'child': [], 'parent': leaf_node_id,\n",
    "                                           'total_node_visits': 0, 'total_node_wins': 0}\n",
    "\n",
    "                    if check_result(new_board, player_mark, self.config):\n",
    "                        best_action = action\n",
    "                        is_availaible = True\n",
    "\n",
    "                self.tree[leaf_node_id]['child'] = childs\n",
    "\n",
    "                if is_availaible:\n",
    "                    child_node_id = best_action\n",
    "                else:\n",
    "                    child_node_id = random.choice(childs)\n",
    "\n",
    "            return leaf_node_id + (child_node_id,)\n",
    "\n",
    "        def simulation(self, child_node_id):\n",
    "            '''\n",
    "            Aim - Reach the final state of the game\n",
    "            '''\n",
    "            self.total_parent_node_visits += 1\n",
    "            state = self.tree[child_node_id]['state']\n",
    "            previous_player = self.tree[child_node_id]['player']\n",
    "\n",
    "            is_terminal = check_result(state, previous_player, self.config)\n",
    "            winning_player = previous_player\n",
    "            count = 0\n",
    "\n",
    "            while not is_terminal:\n",
    "\n",
    "                current_board = np.asarray(state).reshape(\n",
    "                    config.rows*config.columns)\n",
    "                self.actions_available = [c for c in range(\n",
    "                    self.config.columns) if not current_board[c]]\n",
    "\n",
    "                if not len(self.actions_available) or count == 3:\n",
    "                    winning_player = None\n",
    "                    is_terminal = True\n",
    "\n",
    "                else:\n",
    "                    count += 1\n",
    "                    if previous_player == 1:\n",
    "                        current_player = 2\n",
    "                    else:\n",
    "                        current_player = 1\n",
    "\n",
    "                    for actions in self.actions_available:\n",
    "                        state = put_new_piece(\n",
    "                            state, actions, current_player, self.config)\n",
    "                        result = check_result(\n",
    "                            state, current_player, self.config)\n",
    "                        if result:  # A player won the game\n",
    "                            is_terminal = True\n",
    "                            winning_player = current_player\n",
    "                            break\n",
    "\n",
    "                previous_player = current_player\n",
    "\n",
    "            return winning_player\n",
    "\n",
    "        def backpropagation(self, child_node_id, winner):\n",
    "            '''\n",
    "            Aim - Update the traversed nodes\n",
    "            '''\n",
    "            player = self.tree[(0,)]['player']\n",
    "\n",
    "            if winner == None:\n",
    "                reward = 0\n",
    "            elif winner == player:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -10\n",
    "\n",
    "            node_id = child_node_id\n",
    "            self.tree[node_id]['total_node_visits'] += 1\n",
    "            self.tree[node_id]['total_node_wins'] += reward\n",
    "\n",
    "        def start_the_game(self):\n",
    "            '''\n",
    "            Aim - Complete MCTS iteration with all the process running for some fixed time\n",
    "            '''\n",
    "            self.initial_time = time.time()\n",
    "            is_expanded = False\n",
    "\n",
    "            while time.time() - self.initial_time < self.time_limit:\n",
    "                node_id = self.selection()\n",
    "                if not is_expanded:\n",
    "                    node_id = self.expansion(node_id)\n",
    "                    is_expanded = True\n",
    "                winner = self.simulation(node_id)\n",
    "                self.backpropagation(node_id, winner)\n",
    "\n",
    "            current_state_node_id = (0,)\n",
    "            action_candidates = self.tree[current_state_node_id]['child']\n",
    "            total_visits = -math.inf\n",
    "            for action in action_candidates:\n",
    "                action = current_state_node_id + (action,)\n",
    "                visit = self.tree[action]['total_node_visits']\n",
    "                if visit > total_visits:\n",
    "                    total_visits = visit\n",
    "                    best_action = action\n",
    "\n",
    "            return best_action\n",
    "\n",
    "    my_agent = MCTS(obs, config)\n",
    "\n",
    "    return my_agent.start_the_game()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-cyprus",
   "metadata": {},
   "source": [
    "### Validate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.run([connectx_agent, 'negamax'])\n",
    "x = env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-determination",
   "metadata": {},
   "source": [
    "### Задание:\n",
    "Реализовать UCB стратегию и сравнить результат с жадной и эпсилон-жадной\n",
    "<img src = \"https://miro.medium.com/max/6000/1*WcFwHS3n2nXcWQILZqZ5Gw.png\" style=\"width: 500px;\">\n",
    "\n",
    "<!-- <img src = \"https://miro.medium.com/max/1725/1*K5v2P5A7p9f35d_nFcwb9A.png\" style=\"width: 500px;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-assets",
   "metadata": {},
   "source": [
    "## Часть 5: AlphaGo Zero как идея и что в нем особенного"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-workshop",
   "metadata": {},
   "source": [
    "### AlphaGo\n",
    "В общем и целом, AlphaGo опирается на MCTS. Ключевое отличие — на этапе симуляции при добавлении нового узла, для определения насколько она хорошая, вместо случайных rollout'ов используется нейросеть.\n",
    "\n",
    " 1. Тренируюся две сети, каждая из которых получает на вход состояние доски и говорит, какой бы ход в этой ситуации сделал человек. Почему две? Потому что одна — медленная, но работает хорошо (57% верных предсказаний, и каждый дополнительный процент даёт очень солидный бонус к итоговому результату), а вторая обладает намного меньшей точностью, зато быстрая. Обе эти сети тренируюся на демонстрация, полученных из партий сильных игроков.\n",
    "\n",
    " 2. После чего обе сети играют между собой.\n",
    "\n",
    " 3. Тренируется value-сеть, которая получает на вход текущее состояние доски, а в ответ отдаёт число от -1 до 1 — вероятность выиграть, оказавшись в этой позиции в какой-то момент партии.\n",
    "\n",
    "В итоге получается что есть одна медленная и точная функция, которая говорит, куда надо ходить , одна быстрая функция, которая делает то же самое, хоть и не так хорошо, и третья функция, которая, глядя на доску, говорит, проиграешь ты или выиграешь, если окажешься в этой ситуации. Для чего это нужно?\n",
    "Иcпользуем MCTS и используем первую, в качестве стратегии по дереву, вторую — в качестве случайной стратегии, а третью — чтобы напрямую без rollout'а оценить, насколько хорош узел."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-criticism",
   "metadata": {},
   "source": [
    "### Основные проблемы AlphaGo\n",
    "\n",
    " 1. Для стартового обучения используются демонстрации. Получается, что без человеческого интеллекта искусственный интеллект не работает.\n",
    " 2. Много заинженеренных фич. Например, сеть, которая определяет следующий ход, помимо непосредственно текущего состояния получает следующее:\n",
    "     * сколько ходов назад был поставлен тот или иной камень;\n",
    "     * сколько свободных точек вокруг данного камня;\n",
    "     * сколько своих камней ты пожертвуешь, если сходишь в данную точку;\n",
    "     * легален ли вообще данный ход, то есть позволяется ли он правилами го;\n",
    "     * поучаствует ли камень, поставленный в эту точку, в так называемом “лестничном” построении;\n",
    "     * и так далее — в общей сложности 48 слоёв с информацией. А “быстрой” сети, которая предсказывает вероятность победы, и вовсе отдают на вход сто с лишним тысяч заготовленных параметров. Получается, модель учится не играть в го per se, а показывать результаты в некотором заранее очень хорошо подготовленном окружении с большим количеством свойств, о которых ей рассказывает опять же человек.\n",
    "\n",
    " 3. Нужен здоровый кластер, чтобы всё это запустить."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-plymouth",
   "metadata": {},
   "source": [
    "### AlphaGo Zero\n",
    "Изменения, которые произошли в новой версии:\n",
    " 1. Объединили две сети из прошлых версий AlphaGo в одну. Она получает состояние доски с небольшим количеством фич, и в конце два её выхода выдают два результата: policy-выход выдаёт массив 19х19, который показывает, насколько вероятен каждый из ходов из данной позиции, а value выдаёт одно число — вероятность выиграть партию, опять же из данной позиции.\n",
    "\n",
    "2. Поменятли сам RL-алгоритм. Если раньше непосредственно MCTS использовался только во время игры, то теперь он используется сразу при тренировке.\n",
    "\n",
    "<img src='https://habrastorage.org/getpro/habr/post_images/1fd/853/b13/1fd853b13eed794443a5f1d693d0a5f7.png' style=\"width: 500px;\">\n",
    "    \n",
    "В каждом узле дерева состояний хранится четыре значения — **N** (сколько раз был посещен узел), **V** (value), **Q** (усреднённое value всех дочерних узлов) и **P** (вероятность, что из всех допустимых на данном ходу узлов будет выбран именно этот). Алгоритм:\n",
    "\n",
    "* Берётся дерево, корнем которого является текущий узел.\n",
    "* Выбираетсятот дочерний узел, где больше значение **Q + U** (**U** — параметр, стимулирующий исследовани).\n",
    "* После достижения конца дерева — состояния, когда дочерних узлов нет, а игра ещё не закончена, это состояние передается на вход нейросети, в ответ получает **v** (value текущей ноды) и **p** (вероятности следующих ходов).\n",
    "* **v** записывается в узел.\n",
    "* Создаются дочерние узлы с **P** согласно **p** и нулевыми **N**, **V** и **Q**.\n",
    "* Обновляются все узлы выше текущего, которые были выбраны во время симуляции, следующим образом: **N := N + 1**; **V := V + v**; **Q := V / N**.\n",
    "\n",
    "\n",
    "А дальше ход, который сеть действительно сделает, выбирается одним из двух способов:\n",
    "\n",
    "— Если это реальная игра, то выбираем узела, где больше **N**;\n",
    "— Если просто тренировка, выбираем ход из распределения **Pi ~ N ^ (1/T)**, где **T** — просто некоторая температура для контроля баланса между исследованием и эффективностью.\n",
    "\n",
    "<img src='https://habrastorage.org/getpro/habr/post_images/abd/13c/3a8/abd13c3a88e0b76740ca83b5fb21a982.png' style=\"width: 500px;\">\n",
    "Если подробнее, то предположим что имеется некоторая «наилучшая» сеть с весами А. Эта сеть A играет сама с собой 25 000 раз (используя MCTS со своими весами для оценки новых узлов), и для каждого хода сохраняется состояние, распределение $\\Pi$ и то, чем закончилась игра (+1 за победу и -1 за поражение). Дальше батчи из 2048 случайных позиций из последних 500 000 игр, отдаём 1000 таких батчей на тренировку и получаем некоторую новую сеть с весами B, после чего сеть A играет 400 игр с сетью B — при этом обе сети используют MCTS для выбора хода, только при оценке новой ноды A, очевидно, использует свои веса, а B — свои. Если B побеждает более, чем в 55% случаев, она становится лучшей сетью, если нет — чемпион остаётся прежним."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-romantic",
   "metadata": {},
   "source": [
    "## Часть 6: Глубокое планирование как новое направление"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-services",
   "metadata": {},
   "source": [
    "Важными недостатками MCTS подхода являются:\n",
    " 1. Сложность распараллеливания;\n",
    " 2. Отсутствие end-to-end обучения, что значительно сказывается на скорости обучения;\n",
    " 3. Тяжело работать с непрерывными действиями(как и с большим пространством действий);\n",
    " 4. Слабые результаты при большом пространстве состояний.\n",
    "\n",
    "### PlaNet\n",
    "\n",
    "В 2018 году Google AI в сотрудничестве с DeepMind представили алгоритм Deep Planning Network (PlaNet), который изучает модель мира на основе входных изображений и успешно использует ее для планирования. PlaNet решает множество задач управления на основе изображений, конкурируя с продвинутыми агентами без моделей с точки зрения конечной производительности, при этом обеспечивая большую эффективность обработки данных.\n",
    "\n",
    "PlaNet изучает динамическую модель на основе входных данных изображения и эффективно планирует с ее помощью для получения нового опыта. Основной идеей является использование скрытых состояний. Вместо планирования по изображениям используется скрытый слой, таким образом, агент может автоматически изучать более абстрактные представления, такие как положения и скорости объектов, что упрощает планирование.\n",
    "<img src='https://1.bp.blogspot.com/-MDnZ7uyo-VQ/XGX13O1eahI/AAAAAAAADwg/_ObesnP5S_8Tx5nwrzdigKlIX2raW8OewCLcBGAs/s1600/image1.png' style=\"width: 500px;\">\n",
    "\n",
    "Кодирование и декодирование изображений (трапеции на рисунке выше) требует значительных вычислений, что замедляет планирование. Однако в пространстве скрытых состояний планирование происходит значительно быстрее, ввиду того, что для оценки последовательности действий необходимо предсказывать только награды. \n",
    "\n",
    "<img src='https://3.bp.blogspot.com/-XM_o5v3AOB4/XGX2VKrI8HI/AAAAAAAADwo/preHUCFZ1IAp2qmtaSabre3Pzo9wcr7GACLcBGAs/s1600/image5.png' style=\"width: 500px;\">\n",
    "\n",
    "Код и примеры для запуска:\n",
    "https://github.com/google-research/planet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-description",
   "metadata": {},
   "source": [
    "### Dreamer\n",
    "\n",
    "Планирование предложенное в PlaNet требовательно к вычислениям и не полностью использует модель динамики среды. \n",
    "Более того, даже глубокие модели мира ограничены в том, насколько далеко вперед они могут точно предсказать. Dreamer преодолевает эти ограничения, используя value-network и actor-network.\n",
    "\n",
    "Dreamer учит actor-network предсказывать успешные действия. Благодаря этому Dreamer понимает как небольшие изменения в действиях влияют на то, какие награды предсказываются в будущем, позволяя ему усовершенствовать actor-network в направлении, которое больше всего увеличивает награды. Чтобы учесть награды за пределами горизонта прогнозирования, value-network оценивает сумму будущих вознаграждений для каждого состояния модели.\n",
    "\n",
    "<img src='https://1.bp.blogspot.com/-wI4n0f2_2ZQ/XnFFcl9OjqI/AAAAAAAAFgE/dxkupxZvxr0swvc4Qlc6BBuXgGpLdF6PQCEwYBhgL/s1600/image5.gif' style=\"width: 500px;\">\n",
    "\n",
    "<!-- Dreamer учится дальновидному поведению на основе предсказанных последовательностей состояний модели. Сначала он изучает долгосрочное значение (v̂2 – v̂3) каждого состояния, а затем прогнозирует действия (â1 – â2), которые приводят к высоким вознаграждениям и значениям, путем обратного распространения их через последовательность состояний в actor-network. -->\n",
    "\n",
    "Dreamer отличается от PlaNet, не смотря на идентичную модель среды.  PlaNet ищет лучшее действие среди множества прогнозов для различных последовательностей действий. Dreamer же, напротив, не использует этот дорогостоящий поиск, разделяя планирование и действие. После того, как actor-network обучена, она вычисляет действия для взаимодействия со средой без дополнительного поиска. Кроме того, Dreamer рассматривает вознаграждения за пределами горизонта планирования с помощью value-network и использует обратное распространение для эффективного планирования.\n",
    "\n",
    "Код и примеры для запуска:\n",
    "https://github.com/google-research/dreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-content",
   "metadata": {},
   "source": [
    "## Источники\n",
    "1. https://pranav-agarwal-2109.medium.com/game-ai-learning-to-play-connect-4-using-monte-carlo-tree-search-f083d7da451e\n",
    "2. https://habr.com/ru/post/343590/\n",
    "3. https://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html\n",
    "4. https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n",
    "5. https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "6. https://link.springer.com/content/pdf/10.1007/11871842_29.pdf\n",
    "7. https://www.kaggle.com/c/connectx\n",
    "8. https://arxiv.org/abs/2012.04603"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
